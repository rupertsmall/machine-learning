{"name":"Machine-learning","tagline":"machine learning algorithms","body":"##### Table of Contents\r\n[K-means Clustering](#Kmeans)\r\n\r\n[Logistic Regression](#logisticRegression)\r\n\r\n***\r\n\r\n<a name=\"Kmeans\"/>\r\n## K-means Clustering\r\nK-means clustering is one of many machine learning models which challenges a computer to discover some feature of the data \"by itself\". Here the feature is the property of belonging to a certain group or cluster. Data with 5 clusters may look something like this:\r\n\r\n![data clusters](https://cloud.githubusercontent.com/assets/7373621/8417723/38990686-1ea6-11e5-9398-a787e6e21f7f.jpg)\r\n\r\nThis is just randomly generated synthetic data generated using [create_k_cluster.m](https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/create_k_cluster.m). Although its simple to see using the human eye where the clusters are, the goal is to teach the computer how to identify any clusters that may be presented to it without the aid of a human being. To begin we need the computer to understand how to generate some guesses for where the clusters lie: how to algorithmically select a point from each cluster? One possibility is [seed_k_means.m](https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/seed_k_means.m) which places a rectangular grid over the data and keeps only those points on the grid which are close to the real cluster data (plotted above). The data is randomly generated so the specific plots will look different for you. An example looks like this:\r\n\r\n![clusters with seed guesses](https://cloud.githubusercontent.com/assets/7373621/8417724/38a2d58a-1ea6-11e5-88cc-8ade65a6171d.jpg)\r\n\r\nHere the big black dots are the computer's guesses for where the clusters are and the small coloured dots are the actual data clusters. We ultimately want the computer to identify the clusters by drawing circles around them for example. We want the number of initial guesses to be slightly more than K=5 to ensure the computer makes at least one correct guess for each cluster. As you can see there's alot of white space where the algorithm (rightly) doesn't make any guesses, because there are no real data points there.\r\n\r\n### Running the Code\r\nThe functions [create_k_cluster.m](https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/create_k_cluster.m) and [seed_k_means.m](https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/seed_k_means.m) are called by [k_means_plot.m](https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/k_means_plot.m). Notice in [seed_k_means.m](https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/seed_k_means.m) there are no _for_ loops ! Optimising the distance between guesses and the real cluster data is all done using matrix algebra which Matlab/Octave does extremely fast:\r\n```\r\n[x_candidates y_candidates] = meshgrid(xmin:delta_x:xmax, ymin:delta_y:ymax);\r\nx_candidates = reshape(x_candidates, NN+2*N+1,1);\r\ny_candidates = reshape(y_candidates, NN+2*N+1,1);\r\nxx = x_candidates.^2;\r\nyy = y_candidates.^2;\r\n \r\nxdot = kron(X./(XX + YY), x_candidates./(xx+yy));\r\nydot = kron(Y./(XX + YY), y_candidates./(xx+yy));\r\nxdist = kron(exp(X),exp(-x_candidates)).^2;\r\nydist = kron(exp(Y),exp(-y_candidates)).^2;\r\ndist = ((xdist+ydist)/2 - 1).^2;\r\ndot = xdot + ydot -1;\r\ndot = dot.^2;\r\nmeasure = dist.*dot;\r\nmeasure = reshape(measure, length(x_candidates), length(X));\r\n[closest index] = min(measure);\r\n```\r\nAnother example of using matrix operations instead of _for_ loops can be seen in [this program](https://github.com/rupertsmall/distance-heat-maps/blob/master/get_min_perp.m) for calculating the minimum perpendicular distance between all points in a plane and a curve passing through the plane (see [here](http://rupertsmall.github.io/distance-heat-maps/) for plots).\r\n\r\n### Quality of Results\r\nNow that we have figured out a way to generate the seed guesses for the K-means algorithm we can feed this into the actual algorithm, [k_means.m](https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/k_means.m). Note the above examples had K=5 and N=10, so we had many guesses (31 in fact) for the number of clusters, much more than 5. If we do this however and run [k_means.m](https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/k_means.m) we would get something like the following for the seed guesses\r\n\r\n![seed K=5 N=7](https://cloud.githubusercontent.com/assets/7373621/8439792/2a1bf8f0-1f66-11e5-9016-6b7969ee74c0.jpg)\r\n\r\nand something like this for the actual K-means search:\r\n\r\n![K-means K=5, N=7](https://cloud.githubusercontent.com/assets/7373621/8439799/3377c00a-1f66-11e5-9a74-5a8fe94401db.jpg)\r\n\r\nwhere the **yellow stars** indicate where the algorithm thinks the **centre of mass of the clusters** are. But the algorithm has assumed that _too many_ centres of mass exist - we know there are only five ! (and there are more than 5 stars for sure). To fix this we need to reduce N in [k_means.m](https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/k_means.m). Setting K=5, N=2 for example gives the following seed guesses\r\n\r\n![Seed guesses with K=5, N=2](https://cloud.githubusercontent.com/assets/7373621/8439791/2a15dfb0-1f66-11e5-92b9-a4823ec9a397.jpg)\r\n\r\nand the following K-means search:\r\n\r\n![K-means K=5, N=2](https://cloud.githubusercontent.com/assets/7373621/8439798/33759e9c-1f66-11e5-8abf-8da9ed033304.jpg)\r\n\r\nSo the algorithm found the centres of mass (= **yellow stars**) but the seed guesses for lower N were _much_ worse than for higher N. In fact, the algorithm was very lucky to find the correct centres of mass for each cluster given how bad the initial guesses were (and if you run the algorithm a few times you will see that there's not always such a happy ending). So we can see that [k_means.m](https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/k_means.m) does it's job fine, but that the initial seed guesses provided by [seed_k_means.m](https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/seed_k_means.m) can be problematic.\r\n\r\nCan you improve the code of [seed_k_means.m](https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/seed_k_means.m) to provide better seed guesses?\r\n\r\n***\r\n\r\n<a name=\"logisticRegression\"/>\r\n## Logistic Regression\r\nAnother form of clustering used in machine learning is an IN/OUT form of clustering known as logistic regression. This uses an indicator function (usually the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function)) to determine whether a data-point is within a certain set (1) or outside it (0). There are plenty ways to use logistic regression models in The Real World (e.g. spam filters), but it's also good to understand them because they form, albeit in a multi-dimensional way, the building blocks of neural networks: in neural networks there is an IN/OUT decision being made at _every single node in the network_.\r\n\r\nAs with all algorithms it's really good to try code them using as few _for_ loops as possible, instead using matrix multiplication. Here's the crucial \"machine learning\" part of [logistic_function.m](https://github.com/rupertsmall/machine-learning/blob/master/logistic-regression/logistic_function.m)\r\n\r\n```\r\nwhile error > epsilon\r\n    saved_theta = theta;\r\n    theta = theta - (alpha/dim)*((h(X, theta) - Y')*X);\r\n    error = sum(abs(theta-saved_theta));\r\nend\r\n```\r\n\r\nIf we create some synthetic cluster, say a circle of radius 3 centred at (1,3) the resulting plot would look something like this:\r\n\r\n![circular cluster data](https://cloud.githubusercontent.com/assets/7373621/8486089/38d06724-20fa-11e5-99db-4e4943a6e9d1.jpg)\r\n\r\nThe algorithm is, more or less, trying to replicate the coefficients of the equation of the circle (x-1)^2 + (y-3)^2 = 9 so that an optimisation process leading to the correct result would yield\r\n\r\n```\r\ntheta = 1   -2   -6   0   1   1\r\n```\r\n\r\nwhere -2 is the coefficient of x, -6 the coefficient of y, 0 the coefficient of x*y, 1 the coefficient of x^2 and 1 the coefficient of y^2. You can play around with the algorithm by altering its learning rate (alpha), by changing the size of the training data (N) or by changing the point at which the algorithm quits (epsilon). When I run the algorithm I get\r\n\r\n```\r\ntheta = 1.0000   -2.0397   -6.1035   -0.0031    1.0267    1.0175\r\n```\r\n\r\nwhich is fairly close to the correct answer. You can check the predictions theta would make on a new data set by running it through [check_logistic_output.m](https://github.com/rupertsmall/machine-learning/blob/master/logistic-regression/check_logistic_output.m) which should yield a plot much like the green circle above (if your logistic regression model is correct that is!). What changes would you make to the algorithm to get even closer to the correct value of theta?\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}