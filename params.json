{"name":"Machine-learning","tagline":"machine learning algorithms","body":"### K-means clustering\r\nK-means clustering is one of many machine learning models which challenges a computer to discover some feature of the data \"by itself\". Here the feature is the property of belonging to a certain group or cluster. Data with 5 clusters may look something like this:\r\n\r\n![data clusters](https://cloud.githubusercontent.com/assets/7373621/8417723/38990686-1ea6-11e5-9398-a787e6e21f7f.jpg)\r\n\r\nThis is just randomly generated synthetic data generated using [create_k_cluster.m](https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/create_k_cluster.m). Although its simple to see using the human eye where the clusters are, the goal is to teach the computer how to identify any clusters that may be presented to it without the aid of a human being. To begin we need the computer to understand how to generate some guesses for where the clusters lie: how to algorithmically select a point from each cluster? One possibility is [seed_k_means.m](https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/seed_k_means.m) which places a rectangular grid over the data and keeps only those points on the grid which are close to the real cluster data (plotted above). The data is randomly generated so the specific plots will look different for you. An example looks like this:\r\n\r\n![clusters with seed guesses](https://cloud.githubusercontent.com/assets/7373621/8417724/38a2d58a-1ea6-11e5-88cc-8ade65a6171d.jpg)\r\n\r\nHere the big black dots are the computer's guesses for where the clusters are and the small coloured dots are the actual data clusters. We ultimately want the computer to identify the clusters by drawing circles around them for example. We want the number of initial guesses to be slightly more than K=5 to ensure the computer makes at least one correct guess for each cluster. As you can see there's alot of white space where the algorithm (rightly) doesn't make any guesses, because there are no real data points there.\r\n\r\n### Running the Code\r\nThe functions [create_k_cluster.m](https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/create_k_cluster.m) and [seed_k_means.m](https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/seed_k_means.m) are called by [k_means_plot.m](https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/k_means_plot.m). Notice in [seed_k_means.m](https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/seed_k_means.m) there are no _for_ loops ! Optimising the distance between guesses and the real cluster data is all done using matrix algebra which Matlab/Octave does extremely fast:\r\n```\r\n[x_candidates y_candidates] = meshgrid(xmin:delta_x:xmax, ymin:delta_y:ymax);\r\nx_candidates = reshape(x_candidates, NN+2*N+1,1);\r\ny_candidates = reshape(y_candidates, NN+2*N+1,1);\r\nxx = x_candidates.^2;\r\nyy = y_candidates.^2;\r\n \r\nxdot = kron(X./(XX + YY), x_candidates./(xx+yy));\r\nydot = kron(Y./(XX + YY), y_candidates./(xx+yy));\r\nxdist = kron(exp(X),exp(-x_candidates)).^2;\r\nydist = kron(exp(Y),exp(-y_candidates)).^2;\r\ndist = ((xdist+ydist)/2 - 1).^2;\r\ndot = xdot + ydot -1;\r\ndot = dot.^2;\r\nmeasure = dist.*dot;\r\nmeasure = reshape(measure, length(x_candidates), length(X));\r\n[closest index] = min(measure);\r\n```\r\nAnother example of using matrix operations instead of _for_ loops can be seen in [this program](https://github.com/rupertsmall/distance-heat-maps/blob/master/get_min_perp.m) for calculating the minimum perpendicular distance between all points in a plane and a curve passing through the plane (see [here](http://rupertsmall.github.io/distance-heat-maps/) for plots)\r\n\r\n..work in progress","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}