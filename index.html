<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Machine-learning by rupertsmall</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Machine-learning</h1>
      <h2 class="project-tagline">machine learning algorithms</h2>
      <a href="https://github.com/rupertsmall/machine-learning" class="btn">View on GitHub</a>
      <a href="https://github.com/rupertsmall/machine-learning/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/rupertsmall/machine-learning/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h5>
<a id="table-of-contents" class="anchor" href="#table-of-contents" aria-hidden="true"><span class="octicon octicon-link"></span></a>Table of Contents</h5>

<p><a href="#Kmeans">K-means Clustering</a></p>

<p><a href="#logisticRegression">Logistic Regression</a></p>

<p><a href="#neuralNetworks">Neural Networks</a></p>

<hr>

<p><a name="Kmeans"></a></p>

<h2>
<a id="k-means-clustering" class="anchor" href="#k-means-clustering" aria-hidden="true"><span class="octicon octicon-link"></span></a>K-means Clustering</h2>

<p>K-means clustering is one of many machine learning models which challenges a computer to discover some feature of the data "by itself". Here the feature is the property of belonging to a certain group or cluster. Data with 5 clusters may look something like this:</p>

<p><img src="https://cloud.githubusercontent.com/assets/7373621/8417723/38990686-1ea6-11e5-9398-a787e6e21f7f.jpg" alt="data clusters"></p>

<p>This is just randomly generated synthetic data generated using <a href="https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/create_k_cluster.m">create_k_cluster.m</a>. Although its simple to see using the human eye where the clusters are, the goal is to teach the computer how to identify any clusters that may be presented to it without the aid of a human being. To begin we need the computer to understand how to generate some guesses for where the clusters lie: how to algorithmically select a point from each cluster? One possibility is <a href="https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/seed_k_means.m">seed_k_means.m</a> which places a rectangular grid over the data and keeps only those points on the grid which are close to the real cluster data (plotted above). The data is randomly generated so the specific plots will look different for you. An example looks like this:</p>

<p><img src="https://cloud.githubusercontent.com/assets/7373621/8417724/38a2d58a-1ea6-11e5-88cc-8ade65a6171d.jpg" alt="clusters with seed guesses"></p>

<p>Here the big black dots are the computer's guesses for where the clusters are and the small coloured dots are the actual data clusters. We ultimately want the computer to identify the clusters by drawing circles around them for example. We want the number of initial guesses to be slightly more than K=5 to ensure the computer makes at least one correct guess for each cluster. As you can see there's alot of white space where the algorithm (rightly) doesn't make any guesses, because there are no real data points there.</p>

<h3>
<a id="running-the-code" class="anchor" href="#running-the-code" aria-hidden="true"><span class="octicon octicon-link"></span></a>Running the Code</h3>

<p>The functions <a href="https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/create_k_cluster.m">create_k_cluster.m</a> and <a href="https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/seed_k_means.m">seed_k_means.m</a> are called by <a href="https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/k_means_plot.m">k_means_plot.m</a>. Notice in <a href="https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/seed_k_means.m">seed_k_means.m</a> there are no <em>for</em> loops ! Optimising the distance between guesses and the real cluster data is all done using matrix algebra which Matlab/Octave does extremely fast:</p>

<pre><code>[x_candidates y_candidates] = meshgrid(xmin:delta_x:xmax, ymin:delta_y:ymax);
x_candidates = reshape(x_candidates, NN+2*N+1,1);
y_candidates = reshape(y_candidates, NN+2*N+1,1);
xx = x_candidates.^2;
yy = y_candidates.^2;

xdot = kron(X./(XX + YY), x_candidates./(xx+yy));
ydot = kron(Y./(XX + YY), y_candidates./(xx+yy));
xdist = kron(exp(X),exp(-x_candidates)).^2;
ydist = kron(exp(Y),exp(-y_candidates)).^2;
dist = ((xdist+ydist)/2 - 1).^2;
dot = xdot + ydot -1;
dot = dot.^2;
measure = dist.*dot;
measure = reshape(measure, length(x_candidates), length(X));
[closest index] = min(measure);
</code></pre>

<p>Another example of using matrix operations instead of <em>for</em> loops can be seen in <a href="https://github.com/rupertsmall/distance-heat-maps/blob/master/get_min_perp.m">this program</a> for calculating the minimum perpendicular distance between all points in a plane and a curve passing through the plane (see <a href="http://rupertsmall.github.io/distance-heat-maps/">here</a> for plots).</p>

<h3>
<a id="quality-of-results" class="anchor" href="#quality-of-results" aria-hidden="true"><span class="octicon octicon-link"></span></a>Quality of Results</h3>

<p>Now that we have figured out a way to generate the seed guesses for the K-means algorithm we can feed this into the actual algorithm, <a href="https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/k_means.m">k_means.m</a>. Note the above examples had K=5 and N=10, so we had many guesses (31 in fact) for the number of clusters, much more than 5. If we do this however and run <a href="https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/k_means.m">k_means.m</a> we would get something like the following for the seed guesses</p>

<p><img src="https://cloud.githubusercontent.com/assets/7373621/8439792/2a1bf8f0-1f66-11e5-9016-6b7969ee74c0.jpg" alt="seed K=5 N=7"></p>

<p>and something like this for the actual K-means search:</p>

<p><img src="https://cloud.githubusercontent.com/assets/7373621/8439799/3377c00a-1f66-11e5-9a74-5a8fe94401db.jpg" alt="K-means K=5, N=7"></p>

<p>where the <strong>yellow stars</strong> indicate where the algorithm thinks the <strong>centre of mass of the clusters</strong> are. But the algorithm has assumed that <em>too many</em> centres of mass exist - we know there are only five ! (and there are more than 5 stars for sure). To fix this we need to reduce N in <a href="https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/k_means.m">k_means.m</a>. Setting K=5, N=2 for example gives the following seed guesses</p>

<p><img src="https://cloud.githubusercontent.com/assets/7373621/8439791/2a15dfb0-1f66-11e5-92b9-a4823ec9a397.jpg" alt="Seed guesses with K=5, N=2"></p>

<p>and the following K-means search:</p>

<p><img src="https://cloud.githubusercontent.com/assets/7373621/8439798/33759e9c-1f66-11e5-8abf-8da9ed033304.jpg" alt="K-means K=5, N=2"></p>

<p>So the algorithm found the centres of mass (= <strong>yellow stars</strong>) but the seed guesses for lower N were <em>much</em> worse than for higher N. In fact, the algorithm was very lucky to find the correct centres of mass for each cluster given how bad the initial guesses were (and if you run the algorithm a few times you will see that there's not always such a happy ending). So we can see that <a href="https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/k_means.m">k_means.m</a> does it's job fine, but that the initial seed guesses provided by <a href="https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/seed_k_means.m">seed_k_means.m</a> can be problematic.</p>

<p>Can you improve the code of <a href="https://github.com/rupertsmall/machine-learning/blob/master/K-means-clustering/seed_k_means.m">seed_k_means.m</a> to provide better seed guesses?</p>

<hr>

<p><a name="logisticRegression"></a></p>

<h2>
<a id="logistic-regression" class="anchor" href="#logistic-regression" aria-hidden="true"><span class="octicon octicon-link"></span></a>Logistic Regression</h2>

<p>Another form of clustering used in machine learning is an IN/OUT form of clustering known as logistic regression. This uses an indicator function (usually the <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid function</a>) to determine whether a data-point is within a certain set (1) or outside it (0). There are plenty ways to use logistic regression models in The Real World (e.g. spam filters), but it's also good to understand them because they form, albeit in a multi-dimensional way, the building blocks of neural networks: in neural networks there is an IN/OUT decision being made at <em>every single node in the network</em>.</p>

<p>As with all algorithms it's really good to try code them using as few <em>for</em> loops as possible, instead using matrix multiplication. Here's the crucial "machine learning" part of <a href="https://github.com/rupertsmall/machine-learning/blob/master/Logistic-Regression/logistic_function.m">logistic_function.m</a></p>

<pre><code>while error &gt; epsilon
    saved_theta = theta;
    theta = theta - (alpha/dim)*((h(X, theta) - Y')*X);
    error = sum(abs(theta-saved_theta));
end
</code></pre>

<p>If we create some synthetic cluster, say a circle of radius 3 centred at (1,3) the resulting plot would look something like this:</p>

<p><img src="https://cloud.githubusercontent.com/assets/7373621/8486089/38d06724-20fa-11e5-99db-4e4943a6e9d1.jpg" alt="circular cluster data"></p>

<p>The algorithm is, more or less, trying to replicate the coefficients of the equation of the circle (x-1)^2 + (y-3)^2 = 9 so that an optimisation process leading to the correct result would yield</p>

<pre><code>theta = 1   -2   -6   0   1   1
</code></pre>

<p>where -2 is the coefficient of x, -6 the coefficient of y, 0 the coefficient of x*y, 1 the coefficient of x^2 and 1 the coefficient of y^2. You can play around with the algorithm by altering its learning rate (alpha), by changing the size of the training data (N) or by changing the point at which the algorithm quits (epsilon). When I run the algorithm I get</p>

<pre><code>theta = 1.0000   -2.0397   -6.1035   -0.0031    1.0267    1.0175
</code></pre>

<p>which is fairly close to the correct answer. You can check the predictions theta would make on a new data set by running it through <a href="https://github.com/rupertsmall/machine-learning/blob/master/Logistic-Regression/check_logistic_output.m">check_logistic_output.m</a> which should yield a plot much like the green circle above (if your logistic regression model is correct that is!). What changes would you make to the algorithm to get even closer to the correct value of theta?</p>

<p><a name="neuralNetworks"></a></p>

<h2>
<a id="neural-networks" class="anchor" href="#neural-networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Neural Networks</h2>

<p>Neural networks are one of the most powerful type of machine learning model. The basic template is a vector of input nodes + several inner layers (also vectors of nodes) + an output vector of nodes. Each node in one layer is connected to every other node in the next layer through the the same logistic function used in the <a href="#logisticRegression">Logistic Regression</a> section above. The problem then becomes one of optimising the values of the parameter <strong>theta</strong> (see code snippet in previous section) which is now a matrix, in contrast with the simpler logistic regression algorithm. This must be done for <em>each layer of the network</em> and the mathematical structure of the problem requires the optimisation to be carried out backwards(!), starting from the output vector. When training the network you have some training data in the form of input vectors and the corresponding output vectors. First you must</p>

<ol>
<li>
<a href="https://github.com/rupertsmall/machine-learning/blob/master/Neural-Networks/create_MEGA_THETA.m">initialise a guess</a> for <em>theta</em> in each layer. Then for each pair of input/output vectors you need to</li>
<li>
<a href="https://github.com/rupertsmall/machine-learning/blob/master/Neural-Networks/neural_forward.m">run forward propagation</a> by giving the network your input vector and saving the output</li>
<li>then you compare the network's output vector with the true output vector and <a href="https://github.com/rupertsmall/machine-learning/blob/master/Neural-Networks/back_propagation.m">run back propagation</a>
</li>
</ol>

<p>To Be Continued....</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/rupertsmall/machine-learning">Machine-learning</a> is maintained by <a href="https://github.com/rupertsmall">rupertsmall</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>

